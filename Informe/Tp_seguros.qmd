---
format: 
  pdf:
    fig-pos: "H"
    tbl-cap-location: top
lang: es
geometry:
  - top= 25mm
  - left= 18mm
  - right = 18mm
  - bottom = 25mm
  - heightrounded
warning: False
message: False
echo: False
---


::: {.center data-latex=""}

\vspace{3cm}

```{r logo facultad, echo=F, include = T, out.width= "60%"}
knitr::include_graphics("logounr.png")
```

\pagenumbering{gobble}

\vspace{5cm}

\Large
**LICENCIATURA EN ESTADÍSTICA**

\vspace{1cm}

\Large
**Trabajo Práctico**


\vspace{0.3cm}
\large

*Estadística Actuarial*

\vspace{7cm}

\large

**Autores: Rocio Canteros - Franco Santini**

**Docente: Adrián Wibly**

**2025**
\normalsize
\newpage
\hypersetup{linkcolor = black}
\tableofcontents


\newpage
\pagenumbering{arabic}

:::

```{r, echo =F, warning=F, message=F}
# Librerias
library(dplyr)
library(readxl)
library(ggplot2)
library(lubridate)
library(moments)
library(EnvStats)
library(actuar)
library(MASS)
library(ExtDist)
library(kableExtra)
```

```{r, echo =F, warning=F, message=F}
# Carga de los datos
datos <- read_excel("../Datos/Trabajo Final 2024 Base de Datos .xlsx")
cer <- read_excel("../Datos/CER.xlsx")

datos$Fecha <- lubridate::as_date(datos$Fecha)
cer$Fecha <- lubridate::as_date(cer$Fecha)

set.seed(1248) # Garantizar que los resultados sean reproducibles
options(scipen = 999)
```

```{r, echo =F, warning=F, message=F}
limite = cer$CER[cer$Fecha == (datos$Fecha[3431] + days(45))] 
```

```{r, echo =F, warning=F, message=F}
# Ajuste por el indice CER a 45 dias
indice = numeric(nrow(datos))
for (i in 1:nrow(datos)) {
  indice[i] <- cer$CER[cer$Fecha == (datos$Fecha[i] + days(45))]
}

datos$inflacion <- limite/indice

datos$Cuantia_ajust <- datos$Cuantía * limite/indice
```

# Introducción

Una empresa aseguradora resulta solvente cuando dispone o es capaz de recolectar los recursos necesarios para hacer frente a posibles siniestros no previstos. Sea cual sea la duración del seguro, hay una diferencia entre el momento en que se contrata y el momento en el que se cobra, es ahí donde la capacidad de solvencia de la compañia cobra importancia ya que sirve para que los clientes tengan la certeza de que ante la ocurrencia de un siniestro, esta va a ser capaz de cubrirlo.

En este informe, se buscará determinar el Margen de Solvencia Mínimo para una subcartera de pólizas de seguros de automotores de una determinada compañia, de forma tal que su Probabilidad de Solvencia sea del 99% durante el año 2024. Los datos con los que se cuenta pertenecen al año 2023 entonces, debido al contexto inflacionario del país, para trabajar en el 2024 se realizó un ajuste por inflación a través del indice CER, considerando 45 días de rezago, siguiendo la formula: $Cuantia \ Ajustada = Cuantia \cdot \frac{CER_{12/02/24}}{CER_{fecha \ cuantia + 45 \ dias}}$.  


## Análisis descriptivo

Se realizó un análisis descriptivo de las cuantías ajustadas por inlfación de los siniestros.

```{r, echo =F, warning=F, message=F}
#| fig-cap: "Distribución de las cuantías ajustadas por inflación de los siniestros por mes"
#| label: fig-seg1

cuantia_total <- sum(datos$Cuantia_ajust)

meses <- c("Enero", "Febrero", "Marzo", "Abril", "Mayo", "Junio",
           "Julio", "Agosto", "Septiembre", "Octubre", "Noviembre", "Diciembre")
datos |> 
  mutate(mes = factor(month(Fecha), levels = 1:12, labels = meses)) |> 
  group_by(mes) |>
  summarise(Cuantia_total = sum(Cuantia_ajust, na.rm = TRUE)) |> 
  # La linea de arriba ayuda a que podamos poner el borde negro a la barras, si no
  # las barras quedan como a rayas (No sé explicarme mejor, perdón)
  ggplot() +
  aes(x = mes, y = Cuantia_total / 1e6) + 
  geom_bar(stat = "identity", fill = "darkolivegreen2", color = "black") +
  theme_bw() +
  labs(y = "Cuantia*", 
       caption = "*los valores están expresados en millones de pesos",
       x = "Mes") +
  theme(plot.caption = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 30, vjust = 0.9))

```

```{r, echo =F, warning=F, message=F}
#| fig-cap: "Distribución de los siniestros por mes"
#| label: fig-seg2

datos |> 
  mutate(mes = factor(month(Fecha), levels = 1:12, labels = meses)) |>
  group_by(mes) |> 
  summarize(siniestros = n(),
            total_cuantia = sum(Cuantia_ajust)) |> 
  ggplot() +
  aes(x = mes, y = siniestros) +
  geom_bar(stat = "identity", fill = "darkolivegreen2", color = "black") +
  theme_bw() +
  labs(y = "Siniestros", x = "Mes") +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.9))
```

Se puede observar que el total de Marzo fue el más elevado, tanto para las cunatías como los siniestros; lo que llevaría pensar que se pagaron muchas cuantías por la elevada ocurrencia de siniestros. Por otro lado, Septiembre tuvo un comportamiento inverso: la ocurrencia de siniestros y el total de cuantías a fin de mes fueron menores respecto a los 11 meses restantes.

```{r}
#| fig-cap: "Distribución de la cuantía ajustada por inflación"
#| label: fig-seg-re1

datos |> 
  ggplot() +
  aes(x = Cuantia_ajust) +
  geom_histogram(aes(y = after_stat(density)), bins = 80, col = "black", fill = "darkolivegreen2") +
  theme_bw() +
  labs(y = "Densidad", x = "Cuantía ajustada (en pesos)")
```

```{r}
#| tbl-cap: "Medidas resúmenes de la cuantía ajustada"
#| label: tbl-seg-re1

data.frame(
  media = mean(datos$Cuantia_ajust),
  ds = sd(datos$Cuantia_ajust),
  min = min(datos$Cuantia_ajust),
  p_25 = quantile(datos$Cuantia_ajust, probs = 0.25),
  mediana = median(datos$Cuantia_ajust),
  p_75 = quantile(datos$Cuantia_ajust, probs = 0.75),
  p_95 = quantile(datos$Cuantia_ajust, probs = 0.95),
  max = max(datos$Cuantia_ajust)
) |> 
  `rownames<-`("Cuantía ajustada") |> 
  kable(format = "pipe", 
        digits = 2,
        col.names = c("Media", "Desvío Est.", "Mínimo", "$P_{25}$", "$P_{50}$", "$P_{75}$", "$P_{95}$","Máximo")) |> 
  add_footnote("Los valores están expresados en pesos", notation = "none")
```

Se puede observar en la @fig-seg-re1 que la mayoría de las cuantías se concentran por debajo del millón de pesos, al menos el 95% de las mismas como se puede notar en la @tbl-seg-re1. Además, el valor de la cuantía siniestral total ajustada del año 2023 fue de `r round(cuantia_total/1000000, 2)` millones de pesos.


```{r, echo =F, warning=F, message=F}
# Medidas resumenes de las cuantias
media_datos = mean(datos$Cuantia_ajust) # Promedio
var_datos = var(datos$Cuantia_ajust) # Desvío estandar
asimetria_datos = skewness(datos$Cuantia_ajust) # Coeficiente de asimetría

# Medidas resumenes de los siniestros
siniestros_datos <- datos |> 
  mutate(mes = factor(month(Fecha))) |>
  group_by(mes) |> 
  summarize(siniestros = n(),
            total_cuantia = sum(Cuantia_ajust))
media_siniestros = mean(siniestros_datos$siniestros)
var_siniestros = var(siniestros_datos$siniestros)

resumen <- data.frame(
  x = c("Cuantías", "Siniestros"),
  y = c(round(media_datos, 2),round(media_siniestros, 2) ),
  j = c(round(var_datos/1000000, 2), round(var_siniestros, 2))
)
```


# Metodología

Se cuenta con la información de polizas y siniestros de años anteriores, la cual se utiliza para estimar los parametros de las distribuciones que se aplicarán para simular el número de siniestros.


```{r, echo =F, warning=F, message=F}
registro = data.frame(
  anio = c(2021, 2022, 2023),
  polizas = c(24752, 25348, 25615),
  siniestros = c(3023, 3581, 3431)
)

registro <- registro |> 
  mutate(lambda = round(siniestros/polizas, 4))
```


```{r, echo =F, warning=F, message=F}
#| tbl-cap: "Información de la tasa de siniestros por año"
#| label: tbl-seg1

kable(registro,
      format = "pipe",
      col.names = c("Año", "Pólizas", "Siniestros", "$\\lambda$"))
```


Para lograr el objetivo propuesto -MSM que garantice una probabilidad de solvencia del 99%-,se simula la cartera de polizas de la sigueinte manera: 

1. Se hacen 5000 simulaciones para la cantidad de siniestros que pueden ocurrir en el año, siguiendo una distribución de probabilidad.

2. A partir de cada resultado, se simulan las cuantías individuales de los siniestros, siguiendo una distribución de probabilidad.

3. Se suman las cuantías individuales de cada año simulado, teniendo así las cuantías totales simuladas.
Se obtendran las PP (Primas puras) como la cuantía siniestral media, luego, se asumirá una distribución de probabilidad para las cuantías siniestrales totales y se calculará el valor que acumula 99% de probabilidad, para posteriormente calcular el MSM.

\newpage

## Propuesta 1

-   Distribución de los siniestros: Binomial negativa

Dado que no se consideraba adecuado suponer que la media y la variancia de los siniestros fueran iguales, se propone que la varianza fuera un 10% mayor que la media para poder estimar los parámetros. Esto se realizaba teniendo en cuenta que los siniestros ocurridos en años anteriores variaban alrededor de 300 siniestros respecto del promedio. La estimación se basa en el método de los momentos:

$$
\begin{cases}
E(N) = \lambda \\
V(N) = \lambda \; \cdot \; ( 1 + \frac{\lambda}{h})
\end{cases}
\Rightarrow
\begin{cases}
\hat{\lambda} = 0.1324  \\
\hat{h} = 1.3243
\end{cases}
$$


```{r}
n_polizas <- 25615 
k <- 5000 # 5000 simulaciones
media_lambda = mean(registro$lambda)
h <- ((media_lambda)^2)/(media_lambda*1.1 - media_lambda) # Suponiendo que la variancia es un 10% más que la media
p <- h/(media_lambda + h)


siniestros <- numeric(k)
for (i in 1:k) {
  
  # Obtenemos el numero total de siniestros para toda la cartera
  siniestros[i] <- sum(rnbinom(n = n_polizas, prob = p, size = h))
  }
```


```{r}
#| label: fig-seg3
#| layout: [[50], [50]]
#| fig-cap: "Simulación de los siniestros con la distribución Binomial Negativa"
#| fig-subcap: 
#|   - "Distribución de los siniestros por póliza"
#|   - "Simulación de la cantidad de siniestros totales"

densidad_bn <- dnbinom(x = 0:10 ,prob = p, size = h)

ggplot() +
  aes(x = 0:4, y = densidad_bn[1:5]) +
  geom_bar(stat = "identity", width = 0.2, fill  = "plum2") + 
  theme_bw()+
  labs(y = "Densidad", x = "Siniestros por póliza")

ggplot() +
  aes(x = siniestros) +
  geom_histogram(bins = 40, col = "black", fill = "plum2") +
  labs(y = "Frecuencia", x = "Cantidad de siniestros") +
  theme_bw()
```


```{r}
#| tbl-cap: "Medidas resúmenes de la simulación de los siniestros totales"
#| label: tbl-seg2

data.frame(
  Media = mean(siniestros),
  sd = sd(siniestros),
  min = min(siniestros),
  mediana = median(siniestros),
  max = max(siniestros)
) |> 
  `row.names<-`("Siniestros") |> 
  kable(format = "pipe",
        col.names = c("Media", "Desvío Estándar", "Mínimo", "Mediana", "Máximo"),
        digits = 2)
```

<!-- $$ -->
<!-- \begin{array} {c|c|c} -->
<!--  & \text{Media} & {\begin{array}{c}\text{Desvío} \\ \text{Estándar}\end{array}}  & \text{Mínimo} & \text{Máximo} & \text{Mediana} \\ -->
<!-- \hline -->
<!-- Siniestros & 3391.48 & 60.85 & 3179 & 3609 & 3391 -->
<!-- \end{array} -->
<!-- $$ -->

-   Distribución de la cuantías: Log-Normal

Teniendo en cuenta que los datos de las cuantías individuales son asimétricos, se postula que podrían seguir la distribución planteada.

Los parámetros de la misma, se estiman a través del método de los momentos; partiendo del valor R, definido como $R = \frac{m_2}{(m_1)^2}$ donde $m_1$ es el momento de orden 1 y $m_2$ es el momento de orden 2.

$$
R = 1.2629
\Rightarrow
\begin{cases}
\hat{\mu} = ln(m_1) - \frac{1}{2}ln(R) \\
\hat{\sigma} = \sqrt{ln(R)}
\end{cases}
\Rightarrow
\begin{cases}
\hat{\mu} =  13.0465 \\
\hat{\sigma} = 0.4831
\end{cases}
$$


```{r}
mom1 <- media_datos
mom2 <- var_datos + mom1^2

r <- mom2/(mom1^2)

sigma_est <- sqrt(log(r))
mu_est <- log(mom1) - (1/2)*log(r)
```


```{r}
#| fig-cap: "Distribución de las cuantías individuales ajustadas"
#| label: fig-seg4

datos |> 
  ggplot() +
  aes(x = Cuantia_ajust) +
  geom_histogram(aes(y = after_stat(density)), bins = 80, fill = "grey21") +
  stat_function(fun = dlnorm, args = list(meanlog = mu_est, sdlog = sigma_est), 
                color = "orangered", size = 1) +
  scale_x_continuous(limits = c(0,2000000)) +
  theme_bw() +
  labs(y = "Densidad", x = "Cuantía ajustada (en pesos)")
```

Se puede ver que la distribución Log-Normal estaría siendo adecuada para las cuantías individuales. 



## Propuesta 2

-   Distribución de los siniestros: Binomial negativa

Se plantea el mismo supuesto, respecto a la variancia y la media, que se postuló en la propuesta1

-   Distribución de la cuantías: Gamma 

Considerando que los datos de las cuantías individuales son continuos, positivos y asimétricos, se contempla que la distribución postulada podría ser adecuada para ellos. 

Los parámetros de la misma se estiman mediante el método de los momentos:

$$
\begin{cases}
E(X) = \overline{X} = \alpha \; \cdot \; \beta \\
V(X) = S^2 = \alpha \; \cdot \; \beta^2
\end{cases}
\Rightarrow
\begin{cases}
\hat{\alpha} = \frac{(\overline{X})^2}{S^2} \\
\hat{\beta} = \frac{S^2}{\overline{X}}
\end{cases}
\Rightarrow
\begin{cases}
\hat{\alpha} = 3.8035 \\
\hat{\beta} = 136936.7038
\end{cases}
$$


```{r}
#| fig-cap: "Distribución de las cuantías individuales ajustadas"
#| label: fig-seg5

alpha = (media_datos^2)/(var_datos)
beta = var_datos/media_datos

datos |> 
  ggplot() +
  aes(x = Cuantia_ajust) +
  geom_histogram(aes(y = after_stat(density)), bins = 80, fill = "grey21") +
  stat_function(fun = dgamma, args = list(shape = alpha, scale = beta), 
                color = "springgreen3", size = 1) +
    # stat_function(fun = dlnorm, args = list(meanlog = mu_est, sdlog = sigma_est), 
    #             color = "orangered", size = 1) +
  scale_x_continuous(limits = c(0,2000000)) +
  theme_bw() +
  labs(y = "Densidad", x = "Cuantía ajustada (en pesos)")
```


Al igual que la Log-Normal, la distribución Gamma parecería que se adecua a los datos de las cuantías individuales; con la salvedad de que presenta menor densidad que la Log-Normal en valores chicos de cuantías.

## Propuesta 3

-   Distribución de los siniestros: Poisson 

Pese a lo planteado en las propuestas anteriores, se propone que el número de siniestros seguiría una distribución Poisson; por lo tanto, se esta suponiendo que la media y la variancia son iguales.

$$
E(N) = V(N) = \lambda \; \Rightarrow \; \hat{\lambda} = 0.1324
$$


```{r}
n_polizas <- 25615 
k <- 5000 # 5000 simulaciones
media_lambda = mean(registro$lambda)

siniestros_poisson <- numeric(k)
for (i in 1:k) {
  
  # Obtenemos el numero total de siniestros para toda la cartera
  siniestros_poisson[i] <- sum(rpois(n = n_polizas, lambda = media_lambda))
  }
```

```{r}
#| label: fig-seg6
#| layout: [[50], [50]]
#| fig-cap: "Simulación de los siniestros con la distribución Poisson"
#| fig-subcap: 
#|   - "Distribución de los siniestros por póliza"
#|   - "Simulación de la cantidad de siniestros totales"
 
densidad_poisson <- dpois(x = 0:10 ,lambda = media_lambda)

ggplot() +
  aes(x = 0:4, y = densidad_poisson[1:5]) +
  geom_bar(stat = "identity", width = 0.2, fill  = "plum2") + 
  theme_bw()+
  labs(y = "Densidad", x = "Siniestros por póliza")

ggplot() +
  aes(x = siniestros_poisson) +
  geom_histogram(bins = 40, col = "black", fill = "plum2") +
  labs(y = "Frecuencia", x = "Cantidad de siniestros") +
  theme_bw()
```

```{r}
#| tbl-cap: "Medidas resúmenes de la simulación de los siniestros totales"
#| label: tbl-seg3

data.frame(
  Media = mean(siniestros_poisson),
  sd = sd(siniestros_poisson),
  min = min(siniestros_poisson),
  mediana = median(siniestros_poisson),
  max = max(siniestros_poisson)
) |> 
  `row.names<-`("Siniestros") |> 
  kable(format = "pipe",
        col.names = c("Media", "Desvío Estándar", "Mínimo", "Mediana", "Máximo"),
        digits = 2)
```


<!-- $$ -->
<!-- \begin{array} {c|c|c} -->
<!--  & \text{Media} & {\begin{array}{c}\text{Desvío} \\ \text{Estándar}\end{array}}  & \text{Mínimo} & \text{Máximo} & \text{Mediana} \\ -->
<!-- \hline -->
<!-- Siniestros & 3391.44 & 57.94 & 3166 & 3615 & 3392 -->
<!-- \end{array} -->
<!-- $$ -->

-   Distribución de la cuantías: Log-Normal

Teniendo en cuenta que los datos de las cuantías individuales son asimétricos, se postula que podrían seguir la distribución planteada.

Los parámetros de la misma, se estiman a través del método de los momentos; partiendo del valor R, definido como $R = \frac{m_2}{(m_1)^2}$ donde $m_1$ es el momento de orden 1 y $m_2$ es el momento de orden 2. Las estimaciones serán las mismas que las obtenidas para la propuesta 1:

$$
\begin{cases}
\hat{\mu} =  13.0465 \\
\hat{\sigma} = 0.4831
\end{cases}
$$

```{r}
mom1 <- media_datos
mom2 <- var_datos + mom1^2

r <- mom2/(mom1^2)

sigma_est <- sqrt(log(r))
mu_est <- log(mom1) - (1/2)*log(r)
```


Al ser los mismos parámetros estimados, el ajuste de la distribución Log-Normal será el mismo que para el caso donde el número de siniestros sigue la Binomial Negativa, como se muestra en la @fig-seg4.

En las 3 propuestas se utilizan distintas distribuciones para modelar las cuantías individuales, se puede ver una mejor comparación del ajuste de las mismas en la @fig-seg-anexo1.

## Resultados

Como se mencionó antes, con las simulaciones de las cuantías individuales, se simulan las cuantías totales para luego calcular el Margen de Solvencia Mínimo. Este se define de la siguiente manera: $MSM = y_0 - PR$, donde $y_0$ es el valor de la cuantía total perteneciente al cuartil 0.99 y $PR$ es la prima recargada.

Cabe aclarar que la prima recargada es simplemente la prima pura (PP), que es el promedio de las cuantias siniestrales totales simuladas, más un porcentaje de esa prima: $PR = PP + \alpha \cdot PP$. En el presente trabajo los recargos que se le agregan a la prima pura serán del 1%, 2%, 3%, 4% y 5%.

Además, para cada propuesta se contempla la opción de obtener $y_0$ tanto a través de la distribución Normal Estándar como la distribución Normal Power. En el caso de la Normal Estándar, $y_0 = Z_{99} \cdot DS(Y) + E(Y)$; mientras que para el caso de la Normal Power, $y_0 = Y_{99} \cdot DS(Y) + E(Y)$ con $Y_{99} = Z_{99} + \frac{\gamma}{6} \cdot (Z^2_{99} -1 )$. Siendo $Z_{99} \ \text{e} \ Y_{99}$ los percentiles que acumulan 99% de probabilidad en la distribución Normal y Normal Power respectivamente; y $\gamma$ es el coeficiente de asimetría.


### Propuesta 1: Distribución Binomial Negativa + Distribución Log-Normal

```{r}
# Simulación de las cuantías individualea y totales
media_siniestros_simulados1 <- mean(siniestros)
var_siniestros_simulados1 <- var(siniestros)

cuantia_total_simulada <- numeric(length(siniestros))
esperanzas_simuladas <- numeric(length(siniestros))
variancias_simuladas <- numeric(length(siniestros))
contador <- 0

for(i in siniestros){
  contador <- contador + 1
  pos <- rlnorm(n = i, meanlog = mu_est, sdlog = sigma_est)
  esperanzas_simuladas[contador] <- mean(pos)
  variancias_simuladas[contador] <- var(pos)
  cuantia_total_simulada[contador] <- sum(pos)
}

media_ej1 <- mean(esperanzas_simuladas) # Esperanza del ejercicio
var_ej1 <- mean(variancias_simuladas) # Variancia del ejercicio

media_cuantia_total <- media_ej1 * media_siniestros_simulados1

datos_simulados <- data.frame(
  prop1 = cuantia_total_simulada
)
```


```{r}
z = qnorm(0.99)
y1 = z + (skewness(cuantia_total_simulada)/6)*(z^2 - 1)

prop1 <- data.frame(
  RS = c("1%", "2%", "3%", "4%", "5%"),
  PP = c(mean(cuantia_total_simulada)/1000000, mean(cuantia_total_simulada)/1000000, mean(cuantia_total_simulada)/1000000, mean(cuantia_total_simulada)/1000000, mean(cuantia_total_simulada)/1000000), 
  PR = c(mean(cuantia_total_simulada)*1.01/1000000, mean(cuantia_total_simulada)*1.02/1000000, mean(cuantia_total_simulada)*1.03/1000000, mean(cuantia_total_simulada)*1.04/1000000, mean(cuantia_total_simulada)*1.05/1000000),
  P_N = c(z*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, z*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, z*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, z*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, z*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000)
) |> 
  mutate(MSM_n = P_N - PR) |> 
  mutate(P_NP = c(y1*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, y1*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, y1*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, y1*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000, y1*sd(cuantia_total_simulada)/1000000+mean(cuantia_total_simulada)/1000000)) |> 
  mutate(MSM_np = P_NP - PR)
```

```{r}
# Función para calcular la densidad de Normal Power
dnp <- function(x, mu, sigma, asim) {
  y <- ((x-mu)/sigma) + (asim/6)*(((x-mu)/sigma)^2-1)
  (1/(sqrt(2*pi)*sigma))*exp(-0.5*(((y*sigma+mu)-mu)/sigma)^2)
}
```


```{r}
#| fig-cap: "Distribución de la cuantía total"
#| label: fig-seg7

datos_simulados |> 
  ggplot() +
  aes(x = cuantia_total_simulada/1000000) +
  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = "grey21") +
  labs(x = "Cuantía total (en millones de pesos)", y = "Densidad") +
  scale_x_continuous(breaks = c(1650, 1770, 1900)) +
  geom_function(fun = dnorm, args = list(mean = mean(cuantia_total_simulada)/1000000,
                                         sd = sd(cuantia_total_simulada)/1000000), 
                aes(color = "Normal"), size = 0.7, show.legend = T) +
  geom_function(fun = dnp, args = list(mu = mean(cuantia_total_simulada)/1000000,
                                       sigma = sd(cuantia_total_simulada)/1000000,
                                       asim = skewness(cuantia_total_simulada)),
                aes(color = "Normal Power"),show.legend = T, size = 0.7,
                linetype = "dashed") +
  scale_color_manual(name = "Distribución", values = c("springgreen", "brown")) +
  theme_bw() +
  theme(legend.position = "top")
```


Se puede observar que la distribución Normal Power está levemente movida hacia la izquierda de la distribución Normal, ya que el coeficiente de asimetría es de `r round(skewness(cuantia_total_simulada), 4)`, por lo que podría no haber diferencia entre los MSM obtenidos.


### Propuesta 2: Distribución Binomial Negativa + Distribución Gamma

```{r}
# Simulación de las cuantías individuales y totales
cuantia_total_simulada2 <- numeric(length(siniestros))
esperanzas_simuladas2 <- numeric(length(siniestros))
variancias_simuladas2 <- numeric(length(siniestros))
contador <- 0

for(i in siniestros){
  contador <- contador + 1
  pos <- rgamma(n = i, shape = alpha, scale = beta)
  esperanzas_simuladas2[contador] <- mean(pos)
  variancias_simuladas2[contador] <- var(pos)
  cuantia_total_simulada2[contador] <- sum(pos)
}

media_ej2 <- mean(esperanzas_simuladas2) # Esperanza del ejercicio
var_ej2 <- mean(variancias_simuladas2) # Variancia del ejercicio

media_cuantia_total2 <- media_ej2 * media_siniestros_simulados1

datos_simulados$prop2 <- cuantia_total_simulada2
```


```{r}
z = qnorm(0.99)
y2 = z + (skewness(cuantia_total_simulada2)/6)*(z^2 - 1)

prop2 <- data.frame(
  RS = c("1%", "2%", "3%", "4%", "5%"),
  PP = c(mean(cuantia_total_simulada2)/1000000, mean(cuantia_total_simulada2)/1000000, mean(cuantia_total_simulada2)/1000000, mean(cuantia_total_simulada2)/1000000, mean(cuantia_total_simulada2)/1000000),
  PR = c(mean(cuantia_total_simulada2)*1.01/1000000, mean(cuantia_total_simulada2)*1.02/1000000, mean(cuantia_total_simulada2)*1.03/1000000, mean(cuantia_total_simulada2)*1.04/1000000, mean(cuantia_total_simulada2)*1.05/1000000),
  P_N = c(z*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, z*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, z*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, z*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, z*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000)
) |> 
  mutate(MSM_n = P_N - PR) |> 
  mutate(P_NP = c(y2*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, y2*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, y2*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, y2*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000, y2*sd(cuantia_total_simulada2)/1000000+mean(cuantia_total_simulada2)/1000000)) |> 
  mutate(MSM_np = P_NP - PR)
```

```{r}
#| fig-cap: "Distribución de la cuantía total"
#| label: fig-seg8

datos_simulados |> 
  ggplot() +
  aes(x = cuantia_total_simulada2/1000000) +
  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = "grey21") +
  labs(x = "Cuantía total (en millones de pesos)", y = "Densidad") +
  scale_x_continuous(breaks = c(1650, 1770,1900)) +
  geom_function(fun = dnorm, args = list(mean = mean(cuantia_total_simulada2)/1000000,
                                         sd = sd(cuantia_total_simulada2)/1000000), 
                aes(color = "Normal"), size = 0.7, show.legend = T) +
  geom_function(fun = dnp, args = list(mu = mean(cuantia_total_simulada2)/1000000,
                                       sigma = sd(cuantia_total_simulada2)/1000000,
                                       asim = skewness(cuantia_total_simulada2)),
                aes(color = "Normal Power"),show.legend = T, size = 0.7,
                linetype = "dashed") +
  scale_color_manual(name = "Distribución", values = c("springgreen", "brown")) +
  theme_bw() +
  theme(legend.position = "top")
```

Se puede observar que la distribución Normal Power está movida hacia la izquierda de la distribución Normal, ya que el coeficiente de asimetría es de `r round(skewness(cuantia_total_simulada2),4)`, por lo que habría asimetría no solo positiva sino mayor a la que presenta la propuesta 1 y podría haber diferencia entre los MSM obtenidos.

### Propuesta 3: Distribución Poisson + Distribución Log-Normal

```{r}
# Simulación de las cuantías individuales y totales
media_siniestros_simulados3 <- mean(siniestros_poisson)
var_siniestros_simulados3 <- var(siniestros_poisson)

cuantia_total_simulada3 <- numeric(length(siniestros_poisson))
esperanzas_simuladas3 <- numeric(length(siniestros_poisson))
variancias_simuladas3 <- numeric(length(siniestros_poisson))
contador <- 0

for(i in siniestros_poisson){
  contador <- contador + 1
  pos <- rlnorm(n = i, meanlog = mu_est, sdlog = sigma_est)
  esperanzas_simuladas3[contador] <- mean(pos)
  variancias_simuladas3[contador] <- var(pos)
  cuantia_total_simulada3[contador] <- sum(pos)
}

media_ej3 <- mean(esperanzas_simuladas3) # Esperanza del ejercicio
var_ej3 <- mean(variancias_simuladas3) # Variancia del ejercicio

media_cuantia_total3 <- media_ej3 * media_siniestros_simulados3

datos_simulados$prop3 <- cuantia_total_simulada3
```


```{r}
z = qnorm(0.99)
y3 = z + (skewness(cuantia_total_simulada3)/6)*(z^2 - 1)

prop3 <- data.frame(
  RS = c("1%", "2%", "3%", "4%", "5%"),
  PP = c(mean(cuantia_total_simulada3)/1000000, mean(cuantia_total_simulada3)/1000000, mean(cuantia_total_simulada3)/1000000, mean(cuantia_total_simulada3)/1000000, mean(cuantia_total_simulada3)/1000000),
  PR = c(mean(cuantia_total_simulada3)*1.01/1000000, mean(cuantia_total_simulada3)*1.02/1000000, mean(cuantia_total_simulada3)*1.03/1000000, mean(cuantia_total_simulada3)*1.04/1000000, mean(cuantia_total_simulada3)*1.05/1000000),
  P_N = c(z*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, z*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, z*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, z*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, z*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000)
) |> 
  mutate(MSM_n = P_N - PR) |> 
  mutate(P_NP = c(y3*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, y3*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, y3*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, y3*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000, y3*sd(cuantia_total_simulada3)/1000000+mean(cuantia_total_simulada3)/1000000)) |> 
  mutate(MSM_np = P_NP - PR)
```


```{r}
#| fig-cap: "Distribución de la cuantía total"
#| label: fig-seg9

datos_simulados |> 
  ggplot() +
  aes(x = cuantia_total_simulada3/1000000) +
  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = "grey21") +
  labs(x = "Cuantía total", y = "Densidad") +
  scale_x_continuous(breaks = c(1650, 1770,1890)) +
  geom_function(fun = dnorm, args = list(mean = mean(cuantia_total_simulada3)/1000000,
                                         sd = sd(cuantia_total_simulada3)/1000000), 
                aes(color = "Normal"), size = 0.7, show.legend = T) +
  geom_function(fun = dnp, args = list(mu = mean(cuantia_total_simulada3)/1000000,
                                       sigma = sd(cuantia_total_simulada3)/1000000,
                                       asim = skewness(cuantia_total_simulada3)),
                aes(color = "Normal Power"),show.legend = T, size = 0.7, 
                linetype = "dashed") +
  scale_color_manual(name = "Distribución", values = c("springgreen", "brown")) +
  theme_bw() +
  theme(legend.position = "top")
```

Se puede observar que la distribución Normal Power está levemente movida hacia la derecha de la distribución Normal, ya que el coeficiente de asimetría es de `r round(skewness(cuantia_total_simulada3), 4)` por lo que se estaría teniendo una asimetría negativa y podría no haber diferencia entre los MSM obtenido.

```{r}
#| tbl-cap: "Medidas resúmenes de las simulaciones de la cuantía total"
#| label: tbl-seg4

data.frame(
  min = c(min(cuantia_total_simulada)/1000000, min(cuantia_total_simulada2)/1000000, min(cuantia_total_simulada3)/1000000),
  p25 = c(quantile(cuantia_total_simulada, probs = 0.25)/1000000, quantile(cuantia_total_simulada2, probs = 0.25)/1000000, quantile(cuantia_total_simulada3, probs = 0.25)/1000000),
  p50 = c(quantile(cuantia_total_simulada, probs = 0.5)/1000000, quantile(cuantia_total_simulada2, probs = 0.5)/1000000, quantile(cuantia_total_simulada3, probs = 0.5)/1000000),
  p75 = c(quantile(cuantia_total_simulada, probs = 0.75)/1000000, quantile(cuantia_total_simulada2, probs = 0.75)/1000000, quantile(cuantia_total_simulada3, probs = 0.75)/1000000),
  p99 = c(quantile(cuantia_total_simulada, probs = 0.99)/1000000, quantile(cuantia_total_simulada2, probs = 0.99)/1000000, quantile(cuantia_total_simulada3, probs = 0.99)/1000000),
  max = c(max(cuantia_total_simulada)/1000000, max(cuantia_total_simulada2)/1000000, max(cuantia_total_simulada3)/1000000)) |> 
  `rownames<-`(c("BN+LogNormal", "BN+Gamma", "Pois+LogNormal")) |> 
  kable(format = "pipe", digits = 2, col.names = c("Propuesta", "Mín", "$P_{25}$", "$P_{50}$", "$P_{75}$", "$P_{99}$", "Max")) |> 
  add_footnote("Los valores están expresados en millones de pesos", notation = "none")
```

En la @tbl-seg4, se puede notar qué las distribuciones de las cuantías totales simuladas se comportan de forma similar en las 3 propuestas, siendo el método más conservador "BN+Gamma" dado que el valor que acumula un 99% de probabilidad es más grande que para los métodos restantes.

\newpage

# Conclusión

En última instancia, se calculan los MSM (Margen de Solvencia Mínimo) tanto para la distribución Normal -$MSM_{Normal}$- como para la distribución Normal Power -$MSM_{Normal Power}$-, a partir de la PR (Prima Recargada), que se construye como la PP (Prima Pura) más el RS (Recargo de Seguridad), para tomar una decisión sobre qué propuesta seleccionar para garantizar una probabilidad de solvencia del 99%.  

```{r}
#| tbl-cap: "Resultados de la simulación de la cuantía total"
#| label: tbl-seg5

rbind(prop1, prop2, prop3) |> 
  `rownames<-`(c("BN+LogNormal", "BN+LogNormal ", "BN+LogNormal  ", "BN+LogNormal   ", "BN+LogNormal    ", "BN+Gamma", "BN+Gamma ", "BN+Gamma  ", "BN+Gamma   ", "BN+Gamma    ", "Pois+LogNormal", "Pois+LogNormal ", "Pois+LogNormal  ", "Pois+LogNormal   ", "Pois+LogNormal    ")) |> 
  kable(format = "pipe", digits = 2, col.names = c("Propuesta","RS", "PP", "PR", "$Z_{99}$", "$MSM_{Normal}$", "$Y_{99}$", "$MSM_{Normal Power}$")) |> 
  add_footnote("Los valores están expresados en millones de pesos", notation = "none")
```

En la @tbl-seg5 se puede observar que, el mayor MSM se obtiene usando la propuesta "BN+Gamma" asumiendo una distribución Normal Power y con un porcentaje de recargo de seguridad del 1%, en consecuencia, el menor MSM se obtiene usando la propuesta "Poisson+LogNormal" asumiendo una distribución Normal Power y con un porcentaje de recargo de seguridad del 5%.

Por último, en caso de elegir una propuesta para esta situación, si bien no hay una propuesta que sea la más adecuada, se optaría por la "BN+Gamma" con un porcentaje de recargo de seguridad del 2% y bajo el supuesto de que las cuantía totales se distribuyen como una Normal Power.

\newpage

# Anexo

Todo lo realizado en el trabajo se encuentra en el siguiente repositorio el cuál se accede haciendo [click aquí](https://github.com/Franco-Santini/tp_seguros).

```{r}
#| fig-cap: "Comparación del ajuste de las distribuciones propuestas"
#| label: fig-seg-anexo1

datos |> 
  ggplot() +
  aes(x = Cuantia_ajust) +
  geom_histogram(aes(y = after_stat(density)), bins = 50, fill = "grey21") +
  geom_function(fun = dgamma, args = list(shape = alpha, scale = beta), 
                aes(color = "Gamma"), show.legend = T, size = 1) +
  geom_function(fun = dlnorm, args = list(meanlog = mu_est, sdlog = sigma_est), 
              aes(color = "Log-Normal"), show.legend = T, size = 1) +
  scale_color_manual(name = "Distribución", values = c("springgreen3", "orangered")) +
  scale_x_continuous(limits = c(0,2000000)) +
  labs(y = "Densidad", x = "Cuantía ajustada (en pesos)") +
  theme_bw() + 
  theme(legend.position = "top")
```


